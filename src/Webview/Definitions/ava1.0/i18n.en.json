{
    "LivePipeline": {
        "title": "Live pipeline",
        "description": "Live Pipeline represents an unique instance of a pipeline topology which is used for real-time content ingestion and analysis."
    },
    "LivePipeline.name": {
        "title": "Name",
        "description": "Live pipeline unique identifier.",
        "placeholder": ""
    },
    "LivePipeline.systemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with this object.",
        "placeholder": ""
    },
    "LivePipeline.properties": {
        "title": "Properties",
        "description": "Live pipeline properties.",
        "placeholder": ""
    },
    "LivePipelineProperties": {
        "title": "Live pipeline properties",
        "description": "Live pipeline properties."
    },
    "LivePipelineProperties.description": {
        "title": "Description",
        "description": "An optional description of the live pipeline.",
        "placeholder": ""
    },
    "LivePipelineProperties.topologyName": {
        "title": "Topology name",
        "description": "The reference to an existing pipeline topology defined for real-time content processing. When activated, this live pipeline will process content according to the pipeline topology definition.",
        "placeholder": ""
    },
    "LivePipelineProperties.parameters": {
        "title": "Parameters",
        "description": "List of the instance level parameter values for the user-defined topology parameters. A pipeline can only define or override parameters values for parameters which have been declared in the referenced topology. Topology parameters without a default value must be defined. Topology parameters with a default value can be optionally be overridden.",
        "placeholder": ""
    },
    "LivePipelineProperties.state": {
        "title": "State",
        "description": "Current pipeline state (read-only).",
        "placeholder": ""
    },
    "LivePipelineProperties.state.inactive": {
        "title": "Inactive",
        "description": "The live pipeline is idle and not processing media."
    },
    "LivePipelineProperties.state.activating": {
        "title": "Activating",
        "description": "The live pipeline is transitioning into the active state."
    },
    "LivePipelineProperties.state.active": {
        "title": "Active",
        "description": "The live pipeline is active and able to process media. If your data source is not available, for instance, if your RTSP camera is powered off or unreachable, the pipeline will still be active and periodically retrying the connection. Your Azure subscription will be billed for the duration in which the live pipeline is in the active state."
    },
    "LivePipelineProperties.state.deactivating": {
        "title": "Deactivating",
        "description": "The live pipeline is transitioning into the inactive state."
    },
    "ParameterDefinition": {
        "title": "Parameter definition",
        "description": "Defines the parameter value of an specific pipeline topology parameter. See pipeline topology parameters for more information."
    },
    "ParameterDefinition.name": {
        "title": "Name",
        "description": "Name of the parameter declared in the pipeline topology.",
        "placeholder": ""
    },
    "ParameterDefinition.value": {
        "title": "Value",
        "description": "Parameter value to be applied on this specific live pipeline.",
        "placeholder": ""
    },
    "LivePipelineCollection": {
        "title": "Live pipeline collection",
        "description": "A collection of live pipelines."
    },
    "LivePipelineCollection.value": {
        "title": "Value",
        "description": "List of live pipelines.",
        "placeholder": ""
    },
    "LivePipelineCollection.@continuationToken": {
        "title": "@continuation token",
        "description": "A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response.",
        "placeholder": ""
    },
    "PipelineTopologyCollection": {
        "title": "Pipeline topology collection",
        "description": "A collection of pipeline topologies."
    },
    "PipelineTopologyCollection.value": {
        "title": "Value",
        "description": "List of pipeline topologies.",
        "placeholder": ""
    },
    "PipelineTopologyCollection.@continuationToken": {
        "title": "@continuation token",
        "description": "A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response.",
        "placeholder": ""
    },
    "PipelineTopology": {
        "title": "Pipeline topology",
        "description": "Pipeline topology describes the processing steps to be applied when processing media for a particular outcome. The topology should be defined according to the scenario to be achieved and can be reused across many pipeline instances which share the same processing characteristics. For instance, a pipeline topology which acquires data from a RTSP camera, process it with an specific AI model and stored the data on the cloud can be reused across many different cameras, as long as the same processing should be applied across all the cameras. Individual instance properties can be defined through the use of user-defined parameters, which allow for a topology to be parameterized, thus allowing individual pipelines to refer to different values, such as individual cameras RTSP endpoints and credentials. Overall a topology is composed of the following:\r\n\r\n  - Parameters: list of user defined parameters that can be references across the topology nodes.\r\n  - Sources: list of one or more data sources nodes such as an RTSP source which allows for media to be ingested from cameras.\r\n  - Processors: list of nodes which perform data analysis or transformations.\r\n  -Sinks: list of one or more data sinks which allow for data to be stored or exported to other destinations."
    },
    "PipelineTopology.name": {
        "title": "Name",
        "description": "Pipeline topology unique identifier.",
        "placeholder": ""
    },
    "PipelineTopology.systemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with this object.",
        "placeholder": ""
    },
    "PipelineTopology.properties": {
        "title": "Properties",
        "description": "Pipeline topology properties.",
        "placeholder": ""
    },
    "PipelineTopologyProperties": {
        "title": "Pipeline topology properties",
        "description": "Pipeline topology properties."
    },
    "PipelineTopologyProperties.description": {
        "title": "Description",
        "description": "An optional description of the pipeline topology. It is recommended that the expected use of the topology to be described here.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.parameters": {
        "title": "Parameters",
        "description": "List of the topology parameter declarations. Parameters declared here can be referenced throughout the topology nodes through the use of \"${PARAMETER_NAME}\" string pattern. Parameters can have optional default values and can later be defined in individual instances of the pipeline.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.sources": {
        "title": "Sources",
        "description": "List of the topology source nodes. Source nodes enable external data to be ingested by the pipeline.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.processors": {
        "title": "Processors",
        "description": "List of the topology processor nodes. Processor nodes enable pipeline data to be analyzed, processed or transformed.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.sinks": {
        "title": "Sinks",
        "description": "List of the topology sink nodes. Sink nodes allow pipeline data to be stored or exported.",
        "placeholder": ""
    },
    "SystemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with a resource."
    },
    "SystemData.createdAt": {
        "title": "Created at",
        "description": "Date and time when this resource was first created. Value is represented in UTC according to the ISO8601 date format.",
        "placeholder": ""
    },
    "SystemData.lastModifiedAt": {
        "title": "Last modified at",
        "description": "Date and time when this resource was last modified. Value is represented in UTC according to the ISO8601 date format.",
        "placeholder": ""
    },
    "ParameterDeclaration": {
        "title": "Parameter declaration",
        "description": "Single topology parameter declaration. Declared parameters can and must be referenced throughout the topology and can optionally have default values to be used when they are not defined in the pipeline instances."
    },
    "ParameterDeclaration.name": {
        "title": "Name",
        "description": "Name of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.type": {
        "title": "Type",
        "description": "Type of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.type.string": {
        "title": "String",
        "description": "The parameter's value is a string."
    },
    "ParameterDeclaration.type.secretString": {
        "title": "Secret string",
        "description": "The parameter's value is a string that holds sensitive information."
    },
    "ParameterDeclaration.type.int": {
        "title": "Int",
        "description": "The parameter's value is a 32-bit signed integer."
    },
    "ParameterDeclaration.type.double": {
        "title": "Double",
        "description": "The parameter's value is a 64-bit double-precision floating point."
    },
    "ParameterDeclaration.type.bool": {
        "title": "Bool",
        "description": "The parameter's value is a boolean value that is either true or false."
    },
    "ParameterDeclaration.description": {
        "title": "Description",
        "description": "Description of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.default": {
        "title": "Default",
        "description": "The default value for the parameter to be used if the live pipeline does not specify a value.",
        "placeholder": ""
    },
    "SourceNodeBase": {
        "title": "Source node base",
        "description": "Base class for topology source nodes."
    },
    "SourceNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SourceNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "RtspSource": {
        "title": "RTSP source",
        "description": "RTSP source allows for media from an RTSP camera or generic RTSP server to be ingested into a live pipeline."
    },
    "RtspSource.transport": {
        "title": "Transport",
        "description": "Network transport utilized by the RTSP and RTP exchange: TCP or HTTP. When using TCP, the RTP packets are interleaved on the TCP RTSP connection. When using HTTP, the RTSP messages are exchanged through long lived HTTP connections, and the RTP packages are interleaved in the HTTP connections alongside the RTSP messages.",
        "placeholder": ""
    },
    "RtspSource.transport.http": {
        "title": "HTTP",
        "description": "HTTP transport. RTSP messages are exchanged over long running HTTP requests and RTP packets are interleaved within the HTTP channel."
    },
    "RtspSource.transport.tcp": {
        "title": "TCP",
        "description": "TCP transport. RTSP is used directly over TCP and RTP packets are interleaved within the TCP channel."
    },
    "RtspSource.endpoint": {
        "title": "Endpoint",
        "description": "RTSP endpoint information for Video Analyzer to connect to. This contains the required information for Video Analyzer to connect to RTSP cameras and/or generic RTSP servers.",
        "placeholder": ""
    },
    "IotHubMessageSource": {
        "title": "IoT hub message source",
        "description": "IoT Hub Message source allows for the pipeline to consume messages from the IoT Edge Hub. Messages can be routed from other IoT modules via routes declared in the IoT Edge deployment manifest."
    },
    "IotHubMessageSource.hubInputName": {
        "title": "Hub input name",
        "description": "Name of the IoT Edge Hub input from which messages will be consumed.",
        "placeholder": ""
    },
    "IotHubMessageSink": {
        "title": "IoT hub message sink",
        "description": "IoT Hub Message sink allows for pipeline messages to published into the IoT Edge Hub. Published messages can then be delivered to the cloud and other modules via routes declared in the IoT Edge deployment manifest."
    },
    "IotHubMessageSink.hubOutputName": {
        "title": "Hub output name",
        "description": "Name of the Iot Edge Hub output to which the messages will be published.",
        "placeholder": ""
    },
    "EndpointBase": {
        "title": "Endpoint base",
        "description": "Base class for endpoints."
    },
    "EndpointBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "EndpointBase.credentials": {
        "title": "Credentials",
        "description": "Credentials to be presented to the endpoint.",
        "placeholder": ""
    },
    "EndpointBase.url": {
        "title": "URL",
        "description": "The endpoint URL for Video Analyzer to connect to.",
        "placeholder": ""
    },
    "CredentialsBase": {
        "title": "Credentials base",
        "description": "Base class for credential objects."
    },
    "CredentialsBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "UsernamePasswordCredentials": {
        "title": "Username password credentials",
        "description": "Username and password credentials."
    },
    "UsernamePasswordCredentials.username": {
        "title": "Username",
        "description": "Username to be presented as part of the credentials.",
        "placeholder": ""
    },
    "UsernamePasswordCredentials.password": {
        "title": "Password",
        "description": "Password to be presented as part of the credentials. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests.",
        "placeholder": ""
    },
    "HttpHeaderCredentials": {
        "title": "HTTP header credentials",
        "description": "HTTP header credentials."
    },
    "HttpHeaderCredentials.headerName": {
        "title": "Header name",
        "description": "HTTP header name.",
        "placeholder": ""
    },
    "HttpHeaderCredentials.headerValue": {
        "title": "Header value",
        "description": "HTTP header value. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests.",
        "placeholder": ""
    },
    "UnsecuredEndpoint": {
        "title": "Unsecured endpoint",
        "description": "Unsecured endpoint describes an endpoint that the pipeline can connect to over clear transport (no encryption in transit)."
    },
    "TlsEndpoint": {
        "title": "Tls endpoint",
        "description": "TLS endpoint describes an endpoint that the pipeline can connect to over TLS transport (data is encrypted in transit)."
    },
    "TlsEndpoint.trustedCertificates": {
        "title": "Trusted certificates",
        "description": "List of trusted certificate authorities when authenticating a TLS connection. A null list designates that Azure Video Analyzer's list of trusted authorities should be used.",
        "placeholder": ""
    },
    "TlsEndpoint.validationOptions": {
        "title": "Validation options",
        "description": "Validation options to use when authenticating a TLS connection. By default, strict validation is used.",
        "placeholder": ""
    },
    "CertificateSource": {
        "title": "Certificate source",
        "description": "Base class for certificate sources."
    },
    "CertificateSource.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "TlsValidationOptions": {
        "title": "Tls validation options",
        "description": "Options for controlling the validation of TLS endpoints."
    },
    "TlsValidationOptions.ignoreHostname": {
        "title": "Ignore hostname",
        "description": "When set to 'true' causes the certificate subject name validation to be skipped. Default is 'false'.",
        "placeholder": ""
    },
    "TlsValidationOptions.ignoreSignature": {
        "title": "Ignore signature",
        "description": "When set to 'true' causes the certificate chain trust validation to be skipped. Default is 'false'.",
        "placeholder": ""
    },
    "PemCertificateList": {
        "title": "Pem certificate list",
        "description": "A list of PEM formatted certificates."
    },
    "PemCertificateList.certificates": {
        "title": "Certificates",
        "description": "PEM formatted public certificates. One certificate per entry.",
        "placeholder": ""
    },
    "SinkNodeBase": {
        "title": "Sink node base",
        "description": "Base class for topology sink nodes."
    },
    "SinkNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SinkNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "SinkNodeBase.inputs": {
        "title": "Inputs",
        "description": "An array of upstream node references within the topology to be used as inputs for this node.",
        "placeholder": ""
    },
    "NodeInput": {
        "title": "Node input",
        "description": "Describes an input signal to be used on a pipeline node."
    },
    "NodeInput.nodeName": {
        "title": "Node name",
        "description": "The name of the upstream node in the pipeline which output is used as input of the current node.",
        "placeholder": ""
    },
    "NodeInput.outputSelectors": {
        "title": "Output selectors",
        "description": "Allows for the selection of specific data streams (eg. video only) from another node.",
        "placeholder": ""
    },
    "OutputSelector": {
        "title": "Output selector",
        "description": "Allows for the selection of particular streams from another node."
    },
    "OutputSelector.property": {
        "title": "Property",
        "description": "The property of the data stream to be used as the selection criteria.",
        "placeholder": ""
    },
    "OutputSelector.property.mediaType": {
        "title": "Media type",
        "description": "The stream's MIME type or subtype: audio, video or application"
    },
    "OutputSelector.operator": {
        "title": "Operator",
        "description": "The operator to compare properties by.",
        "placeholder": ""
    },
    "OutputSelector.operator.is": {
        "title": "Is",
        "description": "The property is of the type defined by value."
    },
    "OutputSelector.operator.isNot": {
        "title": "Is not",
        "description": "The property is not of the type defined by value."
    },
    "OutputSelector.value": {
        "title": "Value",
        "description": "Value to compare against.",
        "placeholder": ""
    },
    "FileSink": {
        "title": "File sink",
        "description": "File sink allows for video and audio content to be recorded on the file system on the edge device."
    },
    "FileSink.baseDirectoryPath": {
        "title": "Base directory path",
        "description": "Absolute directory path where media files will be stored.",
        "placeholder": "/var/media/output/"
    },
    "FileSink.fileNamePattern": {
        "title": "File name pattern",
        "description": "File name pattern for creating new files when performing event based recording. The pattern must include at least one system variable.",
        "placeholder": "mySampleFile-${System.TopologyName}-${System.PipelineName}-${System.Runtime.DateTime}"
    },
    "FileSink.maximumSizeMiB": {
        "title": "Maximum size MiB",
        "description": "Maximum amount of disk space that can be used for storing files from this sink. Once this limit is reached, the oldest files from this sink will be automatically deleted.",
        "placeholder": ""
    },
    "VideoCreationProperties": {
        "title": "Video creation properties",
        "description": "Optional video properties to be used in case a new video resource needs to be created on the service. These will not take effect if the video already exists."
    },
    "VideoCreationProperties.title": {
        "title": "Title",
        "description": "Optional video title provided by the user. Value can be up to 256 characters long.",
        "placeholder": ""
    },
    "VideoCreationProperties.description": {
        "title": "Description",
        "description": "Optional video description provided by the user. Value can be up to 2048 characters long.",
        "placeholder": ""
    },
    "VideoCreationProperties.segmentLength": {
        "title": "Segment length",
        "description": "Video segment length indicates the length of individual video files (segments) which are persisted to storage. Smaller segments provide lower archive playback latency but generate larger volume of storage transactions. Larger segments reduce the amount of storage transactions while increasing the archive playback latency. Value must be specified in ISO8601 duration format (i.e. \"PT30S\" equals 30 seconds) and can vary between 30 seconds to 5 minutes, in 30 seconds increments. Changing this value after the video is initially created can lead to errors when uploading media to the archive. Default value is 30 seconds.",
        "placeholder": "PT30S"
    },
    "VideoSink": {
        "title": "Video sink",
        "description": "Video sink allows for video and audio to be recorded to the Video Analyzer service. The recorded video can be played from anywhere and further managed from the cloud. Due to security reasons, a given Video Analyzer edge module instance can only record content to new video entries, or existing video entries previously recorded by the same module. Any attempt to record content to an existing video which has not been created by the same module instance will result in failure to record."
    },
    "VideoSink.videoName": {
        "title": "Video name",
        "description": "Name of a new or existing Video Analyzer video resource used for the media recording.",
        "placeholder": "myVideo001"
    },
    "VideoSink.videoCreationProperties": {
        "title": "Video creation properties",
        "description": "Optional video properties to be used in case a new video resource needs to be created on the service.",
        "placeholder": ""
    },
    "VideoSink.localMediaCachePath": {
        "title": "Local media cache path",
        "description": "Path to a local file system directory for caching of temporary media files. This will also be used to store content which cannot be immediately uploaded to Azure due to Internet connectivity issues.",
        "placeholder": "/var/lib/tmp/"
    },
    "VideoSink.localMediaCacheMaximumSizeMiB": {
        "title": "Local media cache maximum size MiB",
        "description": "Maximum amount of disk space that can be used for caching of temporary media files. Once this limit is reached, the oldest segments of the media archive will be continuously deleted in order to make space for new media, thus leading to gaps in the cloud recorded content.",
        "placeholder": ""
    },
    "ProcessorNodeBase": {
        "title": "Processor node base",
        "description": "Base class for topology processor nodes."
    },
    "ProcessorNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "ProcessorNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "ProcessorNodeBase.inputs": {
        "title": "Inputs",
        "description": "An array of upstream node references within the topology to be used as inputs for this node.",
        "placeholder": ""
    },
    "MotionDetectionProcessor": {
        "title": "Motion detection processor",
        "description": "Motion detection processor allows for motion detection on the video stream. It generates motion events whenever motion is present on the video."
    },
    "MotionDetectionProcessor.sensitivity": {
        "title": "Sensitivity",
        "description": "Motion detection sensitivity: low, medium, high.",
        "placeholder": ""
    },
    "MotionDetectionProcessor.sensitivity.low": {
        "title": "Low",
        "description": "Low sensitivity."
    },
    "MotionDetectionProcessor.sensitivity.medium": {
        "title": "Medium",
        "description": "Medium sensitivity."
    },
    "MotionDetectionProcessor.sensitivity.high": {
        "title": "High",
        "description": "High sensitivity."
    },
    "MotionDetectionProcessor.outputMotionRegion": {
        "title": "Output motion region",
        "description": "Indicates whether the processor should detect and output the regions within the video frame where motion was detected. Default is true.",
        "placeholder": ""
    },
    "MotionDetectionProcessor.eventAggregationWindow": {
        "title": "Event aggregation window",
        "description": "Time window duration on which events are aggregated before being emitted. Value must be specified in ISO8601 duration format (i.e. \"PT2S\" equals 2 seconds). Use 0 seconds for no aggregation. Default is 1 second.",
        "placeholder": ""
    },
    "ObjectTrackingProcessor": {
        "title": "Object tracking processor",
        "description": "Object tracker processor allows for continuous tracking of one of more objects over a finite sequence of video frames. It must be used downstream of an object detector extension node, thus allowing for the extension to be configured to to perform inferences on sparse frames through the use of the 'maximumSamplesPerSecond' sampling property. The object tracker node will then track the detected objects over the frames in which the detector is not invoked resulting on a smother tracking of detected objects across the continuum of video frames. The tracker will stop tracking objects which are not subsequently detected by the upstream detector on the subsequent detections."
    },
    "ObjectTrackingProcessor.accuracy": {
        "title": "Accuracy",
        "description": "Object tracker accuracy: low, medium, high. Higher accuracy leads to higher CPU consumption in average.",
        "placeholder": ""
    },
    "ObjectTrackingProcessor.accuracy.low": {
        "title": "Low",
        "description": "Low accuracy."
    },
    "ObjectTrackingProcessor.accuracy.medium": {
        "title": "Medium",
        "description": "Medium accuracy."
    },
    "ObjectTrackingProcessor.accuracy.high": {
        "title": "High",
        "description": "High accuracy."
    },
    "LineCrossingProcessor": {
        "title": "Line crossing processor",
        "description": "Line crossing processor allows for the detection of tracked objects moving across one or more predefined lines. It must be downstream of an object tracker of downstream on an AI extension node that generates sequenceId for objects which are tracked across different frames of the video. Inference events are generated every time objects crosses from one side of the line to another."
    },
    "LineCrossingProcessor.lines": {
        "title": "Lines",
        "description": "An array of lines used to compute line crossing events.",
        "placeholder": ""
    },
    "ExtensionProcessorBase": {
        "title": "Extension processor base",
        "description": "Base class for pipeline extension processors. Pipeline extensions allow for custom media analysis and processing to be plugged into the Video Analyzer pipeline."
    },
    "ExtensionProcessorBase.endpoint": {
        "title": "Endpoint",
        "description": "Endpoint details of the pipeline extension plugin.",
        "placeholder": ""
    },
    "ExtensionProcessorBase.image": {
        "title": "Image",
        "description": "Image transformations and formatting options to be applied to the video frame(s) prior submission to the pipeline extension plugin.",
        "placeholder": ""
    },
    "ExtensionProcessorBase.samplingOptions": {
        "title": "Sampling options",
        "description": "Media sampling parameters that define how often media is submitted to the extension plugin.",
        "placeholder": ""
    },
    "GrpcExtension": {
        "title": "gRPC extension",
        "description": "GRPC extension processor allows pipeline extension plugins to be connected to the pipeline through over a gRPC channel. Extension plugins must act as an gRPC server. Please see https://aka.ms/ava-extension-grpc for details."
    },
    "GrpcExtension.dataTransfer": {
        "title": "Data transfer",
        "description": "Specifies how media is transferred to the extension plugin.",
        "placeholder": ""
    },
    "GrpcExtension.extensionConfiguration": {
        "title": "Extension configuration",
        "description": "An optional configuration string that is sent to the extension plugin. The configuration string is specific to each custom extension and it not understood neither validated by Video Analyzer. Please see https://aka.ms/ava-extension-grpc for details.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer": {
        "title": "gRPC extension data transfer",
        "description": "Describes how media is transferred to the extension plugin."
    },
    "GrpcExtensionDataTransfer.sharedMemorySizeMiB": {
        "title": "Shared memory size MiB",
        "description": "The share memory buffer for sample transfers, in mebibytes. It can only be used with the 'SharedMemory' transfer mode.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer.mode": {
        "title": "Mode",
        "description": "Data transfer mode: embedded or sharedMemory.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer.mode.embedded": {
        "title": "Embedded",
        "description": "Media samples are embedded into the gRPC messages. This mode is less efficient but it requires a simpler implementations and can be used with plugins which are not on the same node as the Video Analyzer module."
    },
    "GrpcExtensionDataTransfer.mode.sharedMemory": {
        "title": "Shared memory",
        "description": "Media samples are made available through shared memory. This mode enables efficient data transfers but it requires that the extension plugin to be co-located on the same node and sharing the same shared memory space."
    },
    "HttpExtension": {
        "title": "HTTP extension",
        "description": "HTTP extension processor allows pipeline extension plugins to be connected to the pipeline through over the HTTP protocol. Extension plugins must act as an HTTP server. Please see https://aka.ms/ava-extension-http for details."
    },
    "ImageProperties": {
        "title": "Image properties",
        "description": "Image transformations and formatting options to be applied to the video frame(s)."
    },
    "SamplingOptions": {
        "title": "Sampling options",
        "description": "Defines how often media is submitted to the extension plugin."
    },
    "SamplingOptions.skipSamplesWithoutAnnotation": {
        "title": "Skip samples without annotation",
        "description": "When set to 'true', prevents frames without upstream inference data to be sent to the extension plugin. This is useful to limit the frames sent to the extension to pre-analyzed frames only. For example, when used downstream from a motion detector, this can enable for only frames in which motion has been detected to be further analyzed.",
        "placeholder": ""
    },
    "SamplingOptions.maximumSamplesPerSecond": {
        "title": "Maximum samples per second",
        "description": "Maximum rate of samples submitted to the extension. This prevents an extension plugin to be overloaded with data.",
        "placeholder": ""
    },
    "ImageScale": {
        "title": "Image scale",
        "description": "Image scaling mode."
    },
    "ImageScale.mode": {
        "title": "Mode",
        "description": "Describes the image scaling mode to be applied. Default mode is 'pad'.",
        "placeholder": ""
    },
    "ImageScale.mode.preserveAspectRatio": {
        "title": "Preserve aspect ratio",
        "description": "Preserves the same aspect ratio as the input image. If only one image dimension is provided, the second dimension is calculated based on the input image aspect ratio. When 2 dimensions are provided, the image is resized to fit the most constraining dimension, considering the input image size and aspect ratio."
    },
    "ImageScale.mode.pad": {
        "title": "Pad",
        "description": "Pads the image with black horizontal stripes (letterbox) or black vertical stripes (pillar-box) so the image is resized to the specified dimensions while not altering the content aspect ratio."
    },
    "ImageScale.mode.stretch": {
        "title": "Stretch",
        "description": "Stretches the original image so it resized to the specified dimensions."
    },
    "ImageScale.width": {
        "title": "Width",
        "description": "The desired output image width.",
        "placeholder": ""
    },
    "ImageScale.height": {
        "title": "Height",
        "description": "The desired output image height.",
        "placeholder": ""
    },
    "ImageFormatProperties": {
        "title": "Image format properties",
        "description": "Base class for image formatting properties."
    },
    "ImageFormatProperties.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "ImageFormatRaw": {
        "title": "Image format raw",
        "description": "Raw image formatting."
    },
    "ImageFormatRaw.pixelFormat": {
        "title": "Pixel format",
        "description": "Pixel format to be applied to the raw image.",
        "placeholder": ""
    },
    "ImageFormatRaw.pixelFormat.yuv420p": {
        "title": "Yuv420p",
        "description": "Planar YUV 4:2:0, 12bpp, (1 Cr and Cb sample per 2x2 Y samples)."
    },
    "ImageFormatRaw.pixelFormat.rgb565be": {
        "title": "Rgb565be",
        "description": "Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian."
    },
    "ImageFormatRaw.pixelFormat.rgb565le": {
        "title": "Rgb565le",
        "description": "Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian."
    },
    "ImageFormatRaw.pixelFormat.rgb555be": {
        "title": "Rgb555be",
        "description": "Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian , X=unused/undefined."
    },
    "ImageFormatRaw.pixelFormat.rgb555le": {
        "title": "Rgb555le",
        "description": "Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined."
    },
    "ImageFormatRaw.pixelFormat.rgb24": {
        "title": "Rgb24",
        "description": "Packed RGB 8:8:8, 24bpp, RGBRGB."
    },
    "ImageFormatRaw.pixelFormat.bgr24": {
        "title": "Bgr24",
        "description": "Packed RGB 8:8:8, 24bpp, BGRBGR."
    },
    "ImageFormatRaw.pixelFormat.argb": {
        "title": "Argb",
        "description": "Packed ARGB 8:8:8:8, 32bpp, ARGBARGB."
    },
    "ImageFormatRaw.pixelFormat.rgba": {
        "title": "Rgba",
        "description": "Packed RGBA 8:8:8:8, 32bpp, RGBARGBA."
    },
    "ImageFormatRaw.pixelFormat.abgr": {
        "title": "Abgr",
        "description": "Packed ABGR 8:8:8:8, 32bpp, ABGRABGR."
    },
    "ImageFormatRaw.pixelFormat.bgra": {
        "title": "Bgra",
        "description": "Packed BGRA 8:8:8:8, 32bpp, BGRABGRA."
    },
    "ImageFormatJpeg": {
        "title": "Image format jpeg",
        "description": "JPEG image encoding."
    },
    "ImageFormatJpeg.quality": {
        "title": "Quality",
        "description": "Image quality value between 0 to 100 (best quality).",
        "placeholder": ""
    },
    "ImageFormatBmp": {
        "title": "Image format bmp",
        "description": "BMP image encoding."
    },
    "ImageFormatPng": {
        "title": "Image format png",
        "description": "PNG image encoding."
    },
    "NamedLineBase": {
        "title": "Named line base",
        "description": "Base class for named lines."
    },
    "NamedLineBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "NamedLineBase.name": {
        "title": "Name",
        "description": "Line name. Must be unique within the node.",
        "placeholder": ""
    },
    "NamedLineString": {
        "title": "Named line string",
        "description": "Describes a line configuration."
    },
    "NamedLineString.line": {
        "title": "Line",
        "description": "Point coordinates for the line start and end, respectively. Example: '[[0.3, 0.2],[0.9, 0.8]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner.",
        "placeholder": "[[0.3,0.2],[0.9,0.8]]"
    },
    "NamedPolygonBase": {
        "title": "Named polygon base",
        "description": "Describes the named polygon."
    },
    "NamedPolygonBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "NamedPolygonBase.name": {
        "title": "Name",
        "description": "Polygon name. Must be unique within the node.",
        "placeholder": ""
    },
    "NamedPolygonString": {
        "title": "Named polygon string",
        "description": "Describes a closed polygon configuration."
    },
    "NamedPolygonString.polygon": {
        "title": "Polygon",
        "description": "Point coordinates for the polygon. Example: '[[0.3, 0.2],[0.9, 0.8],[0.7, 0.6]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner.",
        "placeholder": "[[0.3, 0.2],[0.9, 0.8],[0.7, 0.6]]"
    },
    "SignalGateProcessor": {
        "title": "Signal gate processor",
        "description": "A signal gate determines when to block (gate) incoming media, and when to allow it through. It gathers input events over the activationEvaluationWindow, and determines whether to open or close the gate. See https://aka.ms/ava-signalgate for more information."
    },
    "SignalGateProcessor.activationEvaluationWindow": {
        "title": "Activation evaluation window",
        "description": "The period of time over which the gate gathers input events before evaluating them.",
        "placeholder": "PT1.0S"
    },
    "SignalGateProcessor.activationSignalOffset": {
        "title": "Activation signal offset",
        "description": "Signal offset once the gate is activated (can be negative). It determines the how much farther behind of after the signal will be let through based on the activation time. A negative offset indicates that data prior the activation time must be included on the signal that is let through, once the gate is activated. When used upstream of a file or video sink, this allows for scenarios such as recording buffered media prior an event, such as: record video 5 seconds prior motions is detected.",
        "placeholder": "-PT1.0S"
    },
    "SignalGateProcessor.minimumActivationTime": {
        "title": "Minimum activation time",
        "description": "The minimum period for which the gate remains open in the absence of subsequent triggers (events). When used upstream of a file or video sink, it determines the minimum length of the recorded video clip.",
        "placeholder": "PT10S"
    },
    "SignalGateProcessor.maximumActivationTime": {
        "title": "Maximum activation time",
        "description": "The maximum period for which the gate remains open in the presence of subsequent triggers (events). When used upstream of a file or video sink, it determines the maximum length of the recorded video clip.",
        "placeholder": "PT10S"
    },
    "SpatialAnalysisOperationBase": {
        "title": "Spatial analysis operation base",
        "description": "Base class for Azure Cognitive Services Spatial Analysis operations."
    },
    "SpatialAnalysisOperationBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SpatialAnalysisCustomOperation": {
        "title": "Spatial analysis custom operation",
        "description": "Defines a Spatial Analysis custom operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisCustomOperation.extensionConfiguration": {
        "title": "Extension configuration",
        "description": "Custom configuration to pass to the Azure Cognitive Services Spatial Analysis module.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase": {
        "title": "Spatial analysis typed operation base",
        "description": "Base class for Azure Cognitive Services Spatial Analysis typed operations."
    },
    "SpatialAnalysisTypedOperationBase.debug": {
        "title": "Debug",
        "description": "If set to 'true', enables debugging mode for this operation.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.cameraConfiguration": {
        "title": "Camera configuration",
        "description": "Advanced camera configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.detectorNodeConfiguration": {
        "title": "Detector node configuration",
        "description": "Advanced detector node configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.enableFaceMaskClassifier": {
        "title": "Enable face mask classifier",
        "description": "If set to 'true', enables face mask detection for this operation.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase": {
        "title": "Spatial analysis operation event base",
        "description": "Defines the Azure Cognitive Services Spatial Analysis operation eventing configuration."
    },
    "SpatialAnalysisOperationEventBase.threshold": {
        "title": "Threshold",
        "description": "The event threshold.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase.focus": {
        "title": "Focus",
        "description": "The operation focus type.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase.focus.center": {
        "title": "Center",
        "description": "The center of the object."
    },
    "SpatialAnalysisOperationEventBase.focus.bottomCenter": {
        "title": "Bottom center",
        "description": "The bottom center of the object."
    },
    "SpatialAnalysisOperationEventBase.focus.footprint": {
        "title": "Footprint",
        "description": "The footprint."
    },
    "SpatialAnalysisPersonCountEvent": {
        "title": "Spatial analysis person count event",
        "description": "Defines a Spatial Analysis person count operation eventing configuration."
    },
    "SpatialAnalysisPersonCountEvent.trigger": {
        "title": "Trigger",
        "description": "The event trigger type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountEvent.trigger.event": {
        "title": "Event",
        "description": "Event trigger."
    },
    "SpatialAnalysisPersonCountEvent.trigger.interval": {
        "title": "Interval",
        "description": "Interval trigger."
    },
    "SpatialAnalysisPersonCountEvent.outputFrequency": {
        "title": "Output frequency",
        "description": "The event or interval output frequency.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountOperation": {
        "title": "Spatial analysis person count operation",
        "description": "Defines a Spatial Analysis person count operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonCountOperation.zones": {
        "title": "Zones",
        "description": "The list of zones and optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingEvent": {
        "title": "Spatial analysis person zone crossing event",
        "description": "Defines a Spatial Analysis person crossing zone operation eventing configuration."
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType": {
        "title": "Event type",
        "description": "The event type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType.zoneCrossing": {
        "title": "Zone crossing",
        "description": "Zone crossing event type."
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType.zoneDwellTime": {
        "title": "Zone dwell time",
        "description": "Zone dwell time event type."
    },
    "SpatialAnalysisPersonZoneCrossingZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingOperation": {
        "title": "Spatial analysis person zone crossing operation",
        "description": "Defines a Spatial Analysis person zone crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonZoneCrossingOperation.zones": {
        "title": "Zones",
        "description": "The list of zones with optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent": {
        "title": "Spatial analysis person distance event",
        "description": "Defines a Spatial Analysis person distance operation eventing configuration."
    },
    "SpatialAnalysisPersonDistanceEvent.trigger": {
        "title": "Trigger",
        "description": "The event trigger type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.trigger.event": {
        "title": "Event",
        "description": "Event trigger."
    },
    "SpatialAnalysisPersonDistanceEvent.trigger.interval": {
        "title": "Interval",
        "description": "Interval trigger."
    },
    "SpatialAnalysisPersonDistanceEvent.outputFrequency": {
        "title": "Output frequency",
        "description": "The event or interval output frequency.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.minimumDistanceThreshold": {
        "title": "Minimum distance threshold",
        "description": "The minimum distance threshold",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.maximumDistanceThreshold": {
        "title": "Maximum distance threshold",
        "description": "The maximum distance threshold",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceOperation": {
        "title": "Spatial analysis person distance operation",
        "description": "Defines a Spatial Analysis person distance operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonDistanceOperation.zones": {
        "title": "Zones",
        "description": "The list of zones with optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingEvent": {
        "title": "Spatial analysis person line crossing event",
        "description": "Defines a Spatial Analysis person line crossing operation eventing configuration."
    },
    "SpatialAnalysisPersonLineCrossingLineEvents.line": {
        "title": "Line",
        "description": "The named line.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingLineEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingOperation": {
        "title": "Spatial analysis person line crossing operation",
        "description": "Defines a Spatial Analysis person line crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonLineCrossingOperation.lines": {
        "title": "Lines",
        "description": "The list of lines with optional events.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor": {
        "title": "Cognitive services vision processor",
        "description": "A processor that allows the pipeline topology to send video frames to a Cognitive Services Vision extension. Inference results are relayed to downstream nodes."
    },
    "CognitiveServicesVisionProcessor.endpoint": {
        "title": "Endpoint",
        "description": "Endpoint to which this processor should connect.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.image": {
        "title": "Image",
        "description": "Describes the parameters of the image that is sent as input to the endpoint.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.samplingOptions": {
        "title": "Sampling options",
        "description": "Describes the sampling options to be applied when forwarding samples to the extension.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.operation": {
        "title": "Operation",
        "description": "Describes the Spatial Analysis operation to be used in the Cognitive Services Vision processor.",
        "placeholder": ""
    }
}