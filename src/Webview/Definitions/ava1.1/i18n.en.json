{
    "LivePipeline": {
        "title": "Live pipeline",
        "description": "Live Pipeline represents an unique instance of a pipeline topology which is used for real-time content ingestion and analysis."
    },
    "LivePipeline.name": {
        "title": "Name",
        "description": "Live pipeline unique identifier.",
        "placeholder": ""
    },
    "LivePipeline.systemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with this object.",
        "placeholder": ""
    },
    "LivePipeline.properties": {
        "title": "Properties",
        "description": "Live pipeline properties.",
        "placeholder": ""
    },
    "LivePipelineProperties": {
        "title": "Live pipeline properties",
        "description": "Live pipeline properties."
    },
    "LivePipelineProperties.description": {
        "title": "Description",
        "description": "An optional description of the live pipeline.",
        "placeholder": ""
    },
    "LivePipelineProperties.topologyName": {
        "title": "Topology name",
        "description": "The reference to an existing pipeline topology defined for real-time content processing. When activated, this live pipeline will process content according to the pipeline topology definition.",
        "placeholder": ""
    },
    "LivePipelineProperties.parameters": {
        "title": "Parameters",
        "description": "List of the instance level parameter values for the user-defined topology parameters. A pipeline can only define or override parameters values for parameters which have been declared in the referenced topology. Topology parameters without a default value must be defined. Topology parameters with a default value can be optionally be overridden.",
        "placeholder": ""
    },
    "LivePipelineProperties.state": {
        "title": "State",
        "description": "Current pipeline state (read-only).",
        "placeholder": ""
    },
    "LivePipelineProperties.state.inactive": {
        "title": "Inactive",
        "description": "The live pipeline is idle and not processing media."
    },
    "LivePipelineProperties.state.activating": {
        "title": "Activating",
        "description": "The live pipeline is transitioning into the active state."
    },
    "LivePipelineProperties.state.active": {
        "title": "Active",
        "description": "The live pipeline is active and able to process media. If your data source is not available, for instance, if your RTSP camera is powered off or unreachable, the pipeline will still be active and periodically retrying the connection. Your Azure subscription will be billed for the duration in which the live pipeline is in the active state."
    },
    "LivePipelineProperties.state.deactivating": {
        "title": "Deactivating",
        "description": "The live pipeline is transitioning into the inactive state."
    },
    "ParameterDefinition": {
        "title": "Parameter definition",
        "description": "Defines the parameter value of an specific pipeline topology parameter. See pipeline topology parameters for more information."
    },
    "ParameterDefinition.name": {
        "title": "Name",
        "description": "Name of the parameter declared in the pipeline topology.",
        "placeholder": ""
    },
    "ParameterDefinition.value": {
        "title": "Value",
        "description": "Parameter value to be applied on this specific live pipeline.",
        "placeholder": ""
    },
    "LivePipelineCollection": {
        "title": "Live pipeline collection",
        "description": "A collection of live pipelines."
    },
    "LivePipelineCollection.value": {
        "title": "Value",
        "description": "List of live pipelines.",
        "placeholder": ""
    },
    "LivePipelineCollection.@continuationToken": {
        "title": "@continuation token",
        "description": "A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response.",
        "placeholder": ""
    },
    "PipelineTopologyCollection": {
        "title": "Pipeline topology collection",
        "description": "A collection of pipeline topologies."
    },
    "PipelineTopologyCollection.value": {
        "title": "Value",
        "description": "List of pipeline topologies.",
        "placeholder": ""
    },
    "PipelineTopologyCollection.@continuationToken": {
        "title": "@continuation token",
        "description": "A continuation token to be used in subsequent calls when enumerating through the collection. This is returned when the collection results won't fit in a single response.",
        "placeholder": ""
    },
    "PipelineTopology": {
        "title": "Pipeline topology",
        "description": "Pipeline topology describes the processing steps to be applied when processing media for a particular outcome. The topology should be defined according to the scenario to be achieved and can be reused across many pipeline instances which share the same processing characteristics. For instance, a pipeline topology which acquires data from a RTSP camera, process it with an specific AI model and stored the data on the cloud can be reused across many different cameras, as long as the same processing should be applied across all the cameras. Individual instance properties can be defined through the use of user-defined parameters, which allow for a topology to be parameterized, thus allowing individual pipelines to refer to different values, such as individual cameras RTSP endpoints and credentials. Overall a topology is composed of the following:\r\n\r\n  - Parameters: list of user defined parameters that can be references across the topology nodes.\r\n  - Sources: list of one or more data sources nodes such as an RTSP source which allows for media to be ingested from cameras.\r\n  - Processors: list of nodes which perform data analysis or transformations.\r\n  -Sinks: list of one or more data sinks which allow for data to be stored or exported to other destinations."
    },
    "PipelineTopology.name": {
        "title": "Name",
        "description": "Pipeline topology unique identifier.",
        "placeholder": ""
    },
    "PipelineTopology.systemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with this object.",
        "placeholder": ""
    },
    "PipelineTopology.properties": {
        "title": "Properties",
        "description": "Pipeline topology properties.",
        "placeholder": ""
    },
    "PipelineTopologyProperties": {
        "title": "Pipeline topology properties",
        "description": "Pipeline topology properties."
    },
    "PipelineTopologyProperties.description": {
        "title": "Description",
        "description": "An optional description of the pipeline topology. It is recommended that the expected use of the topology to be described here.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.parameters": {
        "title": "Parameters",
        "description": "List of the topology parameter declarations. Parameters declared here can be referenced throughout the topology nodes through the use of \"${PARAMETER_NAME}\" string pattern. Parameters can have optional default values and can later be defined in individual instances of the pipeline.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.sources": {
        "title": "Sources",
        "description": "List of the topology source nodes. Source nodes enable external data to be ingested by the pipeline.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.processors": {
        "title": "Processors",
        "description": "List of the topology processor nodes. Processor nodes enable pipeline data to be analyzed, processed or transformed.",
        "placeholder": ""
    },
    "PipelineTopologyProperties.sinks": {
        "title": "Sinks",
        "description": "List of the topology sink nodes. Sink nodes allow pipeline data to be stored or exported.",
        "placeholder": ""
    },
    "SystemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with a resource."
    },
    "SystemData.createdAt": {
        "title": "Created at",
        "description": "Date and time when this resource was first created. Value is represented in UTC according to the ISO8601 date format.",
        "placeholder": ""
    },
    "SystemData.lastModifiedAt": {
        "title": "Last modified at",
        "description": "Date and time when this resource was last modified. Value is represented in UTC according to the ISO8601 date format.",
        "placeholder": ""
    },
    "ParameterDeclaration": {
        "title": "Parameter declaration",
        "description": "Single topology parameter declaration. Declared parameters can and must be referenced throughout the topology and can optionally have default values to be used when they are not defined in the pipeline instances."
    },
    "ParameterDeclaration.name": {
        "title": "Name",
        "description": "Name of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.type": {
        "title": "Type",
        "description": "Type of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.type.string": {
        "title": "String",
        "description": "The parameter's value is a string."
    },
    "ParameterDeclaration.type.secretString": {
        "title": "Secret string",
        "description": "The parameter's value is a string that holds sensitive information."
    },
    "ParameterDeclaration.type.int": {
        "title": "Int",
        "description": "The parameter's value is a 32-bit signed integer."
    },
    "ParameterDeclaration.type.double": {
        "title": "Double",
        "description": "The parameter's value is a 64-bit double-precision floating point."
    },
    "ParameterDeclaration.type.bool": {
        "title": "Bool",
        "description": "The parameter's value is a boolean value that is either true or false."
    },
    "ParameterDeclaration.description": {
        "title": "Description",
        "description": "Description of the parameter.",
        "placeholder": ""
    },
    "ParameterDeclaration.default": {
        "title": "Default",
        "description": "The default value for the parameter to be used if the live pipeline does not specify a value.",
        "placeholder": ""
    },
    "SourceNodeBase": {
        "title": "Source node base",
        "description": "Base class for topology source nodes."
    },
    "SourceNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SourceNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "RtspSource": {
        "title": "RTSP source",
        "description": "RTSP source allows for media from an RTSP camera or generic RTSP server to be ingested into a live pipeline."
    },
    "RtspSource.transport": {
        "title": "Transport",
        "description": "Network transport utilized by the RTSP and RTP exchange: TCP or HTTP. When using TCP, the RTP packets are interleaved on the TCP RTSP connection. When using HTTP, the RTSP messages are exchanged through long lived HTTP connections, and the RTP packages are interleaved in the HTTP connections alongside the RTSP messages.",
        "placeholder": ""
    },
    "RtspSource.transport.http": {
        "title": "HTTP",
        "description": "HTTP transport. RTSP messages are exchanged over long running HTTP requests and RTP packets are interleaved within the HTTP channel."
    },
    "RtspSource.transport.tcp": {
        "title": "TCP",
        "description": "TCP transport. RTSP is used directly over TCP and RTP packets are interleaved within the TCP channel."
    },
    "RtspSource.endpoint": {
        "title": "Endpoint",
        "description": "RTSP endpoint information for Video Analyzer to connect to. This contains the required information for Video Analyzer to connect to RTSP cameras and/or generic RTSP servers.",
        "placeholder": ""
    },
    "IotHubMessageSource": {
        "title": "IoT hub message source",
        "description": "IoT Hub Message source allows for the pipeline to consume messages from the IoT Edge Hub. Messages can be routed from other IoT modules via routes declared in the IoT Edge deployment manifest."
    },
    "IotHubMessageSource.hubInputName": {
        "title": "Hub input name",
        "description": "Name of the IoT Edge Hub input from which messages will be consumed.",
        "placeholder": ""
    },
    "IotHubMessageSink": {
        "title": "IoT hub message sink",
        "description": "IoT Hub Message sink allows for pipeline messages to published into the IoT Edge Hub. Published messages can then be delivered to the cloud and other modules via routes declared in the IoT Edge deployment manifest."
    },
    "IotHubMessageSink.hubOutputName": {
        "title": "Hub output name",
        "description": "Name of the Iot Edge Hub output to which the messages will be published.",
        "placeholder": ""
    },
    "EndpointBase": {
        "title": "Endpoint base",
        "description": "Base class for endpoints."
    },
    "EndpointBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "EndpointBase.credentials": {
        "title": "Credentials",
        "description": "Credentials to be presented to the endpoint.",
        "placeholder": ""
    },
    "EndpointBase.url": {
        "title": "URL",
        "description": "The endpoint URL for Video Analyzer to connect to.",
        "placeholder": ""
    },
    "CredentialsBase": {
        "title": "Credentials base",
        "description": "Base class for credential objects."
    },
    "CredentialsBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "UsernamePasswordCredentials": {
        "title": "Username password credentials",
        "description": "Username and password credentials."
    },
    "UsernamePasswordCredentials.username": {
        "title": "Username",
        "description": "Username to be presented as part of the credentials.",
        "placeholder": ""
    },
    "UsernamePasswordCredentials.password": {
        "title": "Password",
        "description": "Password to be presented as part of the credentials. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests.",
        "placeholder": ""
    },
    "HttpHeaderCredentials": {
        "title": "HTTP header credentials",
        "description": "HTTP header credentials."
    },
    "HttpHeaderCredentials.headerName": {
        "title": "Header name",
        "description": "HTTP header name.",
        "placeholder": ""
    },
    "HttpHeaderCredentials.headerValue": {
        "title": "Header value",
        "description": "HTTP header value. It is recommended that this value is parameterized as a secret string in order to prevent this value to be returned as part of the resource on API requests.",
        "placeholder": ""
    },
    "UnsecuredEndpoint": {
        "title": "Unsecured endpoint",
        "description": "Unsecured endpoint describes an endpoint that the pipeline can connect to over clear transport (no encryption in transit)."
    },
    "TlsEndpoint": {
        "title": "Tls endpoint",
        "description": "TLS endpoint describes an endpoint that the pipeline can connect to over TLS transport (data is encrypted in transit)."
    },
    "TlsEndpoint.trustedCertificates": {
        "title": "Trusted certificates",
        "description": "List of trusted certificate authorities when authenticating a TLS connection. A null list designates that Azure Video Analyzer's list of trusted authorities should be used.",
        "placeholder": ""
    },
    "TlsEndpoint.validationOptions": {
        "title": "Validation options",
        "description": "Validation options to use when authenticating a TLS connection. By default, strict validation is used.",
        "placeholder": ""
    },
    "SymmetricKeyCredentials": {
        "title": "Symmetric key credentials",
        "description": "Symmetric key credential."
    },
    "SymmetricKeyCredentials.key": {
        "title": "Key",
        "description": "Symmetric key credential.",
        "placeholder": ""
    },
    "CertificateSource": {
        "title": "Certificate source",
        "description": "Base class for certificate sources."
    },
    "CertificateSource.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "TlsValidationOptions": {
        "title": "Tls validation options",
        "description": "Options for controlling the validation of TLS endpoints."
    },
    "TlsValidationOptions.ignoreHostname": {
        "title": "Ignore hostname",
        "description": "When set to 'true' causes the certificate subject name validation to be skipped. Default is 'false'.",
        "placeholder": ""
    },
    "TlsValidationOptions.ignoreSignature": {
        "title": "Ignore signature",
        "description": "When set to 'true' causes the certificate chain trust validation to be skipped. Default is 'false'.",
        "placeholder": ""
    },
    "PemCertificateList": {
        "title": "Pem certificate list",
        "description": "A list of PEM formatted certificates."
    },
    "PemCertificateList.certificates": {
        "title": "Certificates",
        "description": "PEM formatted public certificates. One certificate per entry.",
        "placeholder": ""
    },
    "SinkNodeBase": {
        "title": "Sink node base",
        "description": "Base class for topology sink nodes."
    },
    "SinkNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SinkNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "SinkNodeBase.inputs": {
        "title": "Inputs",
        "description": "An array of upstream node references within the topology to be used as inputs for this node.",
        "placeholder": ""
    },
    "NodeInput": {
        "title": "Node input",
        "description": "Describes an input signal to be used on a pipeline node."
    },
    "NodeInput.nodeName": {
        "title": "Node name",
        "description": "The name of the upstream node in the pipeline which output is used as input of the current node.",
        "placeholder": ""
    },
    "NodeInput.outputSelectors": {
        "title": "Output selectors",
        "description": "Allows for the selection of specific data streams (eg. video only) from another node.",
        "placeholder": ""
    },
    "OutputSelector": {
        "title": "Output selector",
        "description": "Allows for the selection of particular streams from another node."
    },
    "OutputSelector.property": {
        "title": "Property",
        "description": "The property of the data stream to be used as the selection criteria.",
        "placeholder": ""
    },
    "OutputSelector.property.mediaType": {
        "title": "Media type",
        "description": "The stream's MIME type or subtype: audio, video or application"
    },
    "OutputSelector.operator": {
        "title": "Operator",
        "description": "The operator to compare properties by.",
        "placeholder": ""
    },
    "OutputSelector.operator.is": {
        "title": "Is",
        "description": "The property is of the type defined by value."
    },
    "OutputSelector.operator.isNot": {
        "title": "Is not",
        "description": "The property is not of the type defined by value."
    },
    "OutputSelector.value": {
        "title": "Value",
        "description": "Value to compare against.",
        "placeholder": ""
    },
    "FileSink": {
        "title": "File sink",
        "description": "File sink allows for video and audio content to be recorded on the file system on the edge device."
    },
    "FileSink.baseDirectoryPath": {
        "title": "Base directory path",
        "description": "Absolute directory path where media files will be stored.",
        "placeholder": "/var/media/output/"
    },
    "FileSink.fileNamePattern": {
        "title": "File name pattern",
        "description": "File name pattern for creating new files when performing event based recording. The pattern must include at least one system variable.",
        "placeholder": "mySampleFile-${System.TopologyName}-${System.PipelineName}-${System.Runtime.DateTime}"
    },
    "FileSink.maximumSizeMiB": {
        "title": "Maximum size MiB",
        "description": "Maximum amount of disk space that can be used for storing files from this sink. Once this limit is reached, the oldest files from this sink will be automatically deleted.",
        "placeholder": ""
    },
    "VideoPublishingOptions": {
        "title": "Video publishing options",
        "description": "Options for changing video publishing behavior on the video sink and output video."
    },
    "VideoPublishingOptions.enableVideoPreviewImage": {
        "title": "Enable video preview image",
        "description": "When set to 'true' the video will publish preview images. Default is 'false'.",
        "placeholder": ""
    },
    "VideoCreationProperties": {
        "title": "Video creation properties",
        "description": "Optional video properties to be used in case a new video resource needs to be created on the service. These will not take effect if the video already exists."
    },
    "VideoCreationProperties.title": {
        "title": "Title",
        "description": "Optional video title provided by the user. Value can be up to 256 characters long.",
        "placeholder": ""
    },
    "VideoCreationProperties.description": {
        "title": "Description",
        "description": "Optional video description provided by the user. Value can be up to 2048 characters long.",
        "placeholder": ""
    },
    "VideoCreationProperties.segmentLength": {
        "title": "Segment length",
        "description": "Video segment length indicates the length of individual video files (segments) which are persisted to storage. Smaller segments provide lower archive playback latency but generate larger volume of storage transactions. Larger segments reduce the amount of storage transactions while increasing the archive playback latency. Value must be specified in ISO8601 duration format (i.e. \"PT30S\" equals 30 seconds) and can vary between 30 seconds to 5 minutes, in 30 seconds increments. Changing this value after the video is initially created can lead to errors when uploading media to the archive. Default value is 30 seconds.",
        "placeholder": "PT30S"
    },
    "VideoCreationProperties.retentionPeriod": {
        "title": "Retention period",
        "description": "Video retention period indicates how long the video is kept in storage, and must be a multiple of 1 day. For example, if this is set to 30 days, then content older than 30 days will be deleted.",
        "placeholder": "P30D"
    },
    "VideoSink": {
        "title": "Video sink",
        "description": "Video sink allows for video and audio to be recorded to the Video Analyzer service. The recorded video can be played from anywhere and further managed from the cloud. Due to security reasons, a given Video Analyzer edge module instance can only record content to new video entries, or existing video entries previously recorded by the same module. Any attempt to record content to an existing video which has not been created by the same module instance will result in failure to record."
    },
    "VideoSink.videoName": {
        "title": "Video name",
        "description": "Name of a new or existing Video Analyzer video resource used for the media recording.",
        "placeholder": "myVideo001"
    },
    "VideoSink.videoCreationProperties": {
        "title": "Video creation properties",
        "description": "Optional video properties to be used in case a new video resource needs to be created on the service.",
        "placeholder": ""
    },
    "VideoSink.videoPublishingOptions": {
        "title": "Video publishing options",
        "description": "Optional video publishing options to be used for changing publishing behavior of the output video.",
        "placeholder": ""
    },
    "VideoSink.localMediaCachePath": {
        "title": "Local media cache path",
        "description": "Path to a local file system directory for caching of temporary media files. This will also be used to store content which cannot be immediately uploaded to Azure due to Internet connectivity issues.",
        "placeholder": "/var/lib/tmp/"
    },
    "VideoSink.localMediaCacheMaximumSizeMiB": {
        "title": "Local media cache maximum size MiB",
        "description": "Maximum amount of disk space that can be used for caching of temporary media files. Once this limit is reached, the oldest segments of the media archive will be continuously deleted in order to make space for new media, thus leading to gaps in the cloud recorded content.",
        "placeholder": ""
    },
    "ProcessorNodeBase": {
        "title": "Processor node base",
        "description": "Base class for topology processor nodes."
    },
    "ProcessorNodeBase.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "ProcessorNodeBase.name": {
        "title": "Name",
        "description": "Node name. Must be unique within the topology.",
        "placeholder": ""
    },
    "ProcessorNodeBase.inputs": {
        "title": "Inputs",
        "description": "An array of upstream node references within the topology to be used as inputs for this node.",
        "placeholder": ""
    },
    "MotionDetectionProcessor": {
        "title": "Motion detection processor",
        "description": "Motion detection processor allows for motion detection on the video stream. It generates motion events whenever motion is present on the video."
    },
    "MotionDetectionProcessor.sensitivity": {
        "title": "Sensitivity",
        "description": "Motion detection sensitivity: low, medium, high.",
        "placeholder": ""
    },
    "MotionDetectionProcessor.sensitivity.low": {
        "title": "Low",
        "description": "Low sensitivity."
    },
    "MotionDetectionProcessor.sensitivity.medium": {
        "title": "Medium",
        "description": "Medium sensitivity."
    },
    "MotionDetectionProcessor.sensitivity.high": {
        "title": "High",
        "description": "High sensitivity."
    },
    "MotionDetectionProcessor.outputMotionRegion": {
        "title": "Output motion region",
        "description": "Indicates whether the processor should detect and output the regions within the video frame where motion was detected. Default is true.",
        "placeholder": ""
    },
    "MotionDetectionProcessor.eventAggregationWindow": {
        "title": "Event aggregation window",
        "description": "Time window duration on which events are aggregated before being emitted. Value must be specified in ISO8601 duration format (i.e. \"PT2S\" equals 2 seconds). Use 0 seconds for no aggregation. Default is 1 second.",
        "placeholder": ""
    },
    "ObjectTrackingProcessor": {
        "title": "Object tracking processor",
        "description": "Object tracker processor allows for continuous tracking of one of more objects over a finite sequence of video frames. It must be used downstream of an object detector extension node, thus allowing for the extension to be configured to to perform inferences on sparse frames through the use of the 'maximumSamplesPerSecond' sampling property. The object tracker node will then track the detected objects over the frames in which the detector is not invoked resulting on a smother tracking of detected objects across the continuum of video frames. The tracker will stop tracking objects which are not subsequently detected by the upstream detector on the subsequent detections."
    },
    "ObjectTrackingProcessor.accuracy": {
        "title": "Accuracy",
        "description": "Object tracker accuracy: low, medium, high. Higher accuracy leads to higher CPU consumption in average.",
        "placeholder": ""
    },
    "ObjectTrackingProcessor.accuracy.low": {
        "title": "Low",
        "description": "Low accuracy."
    },
    "ObjectTrackingProcessor.accuracy.medium": {
        "title": "Medium",
        "description": "Medium accuracy."
    },
    "ObjectTrackingProcessor.accuracy.high": {
        "title": "High",
        "description": "High accuracy."
    },
    "LineCrossingProcessor": {
        "title": "Line crossing processor",
        "description": "Line crossing processor allows for the detection of tracked objects moving across one or more predefined lines. It must be downstream of an object tracker of downstream on an AI extension node that generates sequenceId for objects which are tracked across different frames of the video. Inference events are generated every time objects crosses from one side of the line to another."
    },
    "LineCrossingProcessor.lines": {
        "title": "Lines",
        "description": "An array of lines used to compute line crossing events.",
        "placeholder": ""
    },
    "ExtensionProcessorBase": {
        "title": "Extension processor base",
        "description": "Base class for pipeline extension processors. Pipeline extensions allow for custom media analysis and processing to be plugged into the Video Analyzer pipeline."
    },
    "ExtensionProcessorBase.endpoint": {
        "title": "Endpoint",
        "description": "Endpoint details of the pipeline extension plugin.",
        "placeholder": ""
    },
    "ExtensionProcessorBase.image": {
        "title": "Image",
        "description": "Image transformations and formatting options to be applied to the video frame(s) prior submission to the pipeline extension plugin.",
        "placeholder": ""
    },
    "ExtensionProcessorBase.samplingOptions": {
        "title": "Sampling options",
        "description": "Media sampling parameters that define how often media is submitted to the extension plugin.",
        "placeholder": ""
    },
    "GrpcExtension": {
        "title": "gRPC extension",
        "description": "GRPC extension processor allows pipeline extension plugins to be connected to the pipeline through over a gRPC channel. Extension plugins must act as an gRPC server. Please see https://aka.ms/ava-extension-grpc for details."
    },
    "GrpcExtension.dataTransfer": {
        "title": "Data transfer",
        "description": "Specifies how media is transferred to the extension plugin.",
        "placeholder": ""
    },
    "GrpcExtension.extensionConfiguration": {
        "title": "Extension configuration",
        "description": "An optional configuration string that is sent to the extension plugin. The configuration string is specific to each custom extension and it not understood neither validated by Video Analyzer. Please see https://aka.ms/ava-extension-grpc for details.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer": {
        "title": "gRPC extension data transfer",
        "description": "Describes how media is transferred to the extension plugin."
    },
    "GrpcExtensionDataTransfer.sharedMemorySizeMiB": {
        "title": "Shared memory size MiB",
        "description": "The share memory buffer for sample transfers, in mebibytes. It can only be used with the 'SharedMemory' transfer mode.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer.mode": {
        "title": "Mode",
        "description": "Data transfer mode: embedded or sharedMemory.",
        "placeholder": ""
    },
    "GrpcExtensionDataTransfer.mode.embedded": {
        "title": "Embedded",
        "description": "Media samples are embedded into the gRPC messages. This mode is less efficient but it requires a simpler implementations and can be used with plugins which are not on the same node as the Video Analyzer module."
    },
    "GrpcExtensionDataTransfer.mode.sharedMemory": {
        "title": "Shared memory",
        "description": "Media samples are made available through shared memory. This mode enables efficient data transfers but it requires that the extension plugin to be co-located on the same node and sharing the same shared memory space."
    },
    "HttpExtension": {
        "title": "HTTP extension",
        "description": "HTTP extension processor allows pipeline extension plugins to be connected to the pipeline through over the HTTP protocol. Extension plugins must act as an HTTP server. Please see https://aka.ms/ava-extension-http for details."
    },
    "ImageProperties": {
        "title": "Image properties",
        "description": "Image transformations and formatting options to be applied to the video frame(s)."
    },
    "SamplingOptions": {
        "title": "Sampling options",
        "description": "Defines how often media is submitted to the extension plugin."
    },
    "SamplingOptions.skipSamplesWithoutAnnotation": {
        "title": "Skip samples without annotation",
        "description": "When set to 'true', prevents frames without upstream inference data to be sent to the extension plugin. This is useful to limit the frames sent to the extension to pre-analyzed frames only. For example, when used downstream from a motion detector, this can enable for only frames in which motion has been detected to be further analyzed.",
        "placeholder": ""
    },
    "SamplingOptions.maximumSamplesPerSecond": {
        "title": "Maximum samples per second",
        "description": "Maximum rate of samples submitted to the extension. This prevents an extension plugin to be overloaded with data.",
        "placeholder": ""
    },
    "ImageScale": {
        "title": "Image scale",
        "description": "Image scaling mode."
    },
    "ImageScale.mode": {
        "title": "Mode",
        "description": "Describes the image scaling mode to be applied. Default mode is 'pad'.",
        "placeholder": ""
    },
    "ImageScale.mode.preserveAspectRatio": {
        "title": "Preserve aspect ratio",
        "description": "Preserves the same aspect ratio as the input image. If only one image dimension is provided, the second dimension is calculated based on the input image aspect ratio. When 2 dimensions are provided, the image is resized to fit the most constraining dimension, considering the input image size and aspect ratio."
    },
    "ImageScale.mode.pad": {
        "title": "Pad",
        "description": "Pads the image with black horizontal stripes (letterbox) or black vertical stripes (pillar-box) so the image is resized to the specified dimensions while not altering the content aspect ratio."
    },
    "ImageScale.mode.stretch": {
        "title": "Stretch",
        "description": "Stretches the original image so it resized to the specified dimensions."
    },
    "ImageScale.width": {
        "title": "Width",
        "description": "The desired output image width.",
        "placeholder": ""
    },
    "ImageScale.height": {
        "title": "Height",
        "description": "The desired output image height.",
        "placeholder": ""
    },
    "ImageFormatProperties": {
        "title": "Image format properties",
        "description": "Base class for image formatting properties."
    },
    "ImageFormatProperties.@type": {
        "title": "@type",
        "description": "Type discriminator for the derived types.",
        "placeholder": ""
    },
    "ImageFormatRaw": {
        "title": "Image format raw",
        "description": "Raw image formatting."
    },
    "ImageFormatRaw.pixelFormat": {
        "title": "Pixel format",
        "description": "Pixel format to be applied to the raw image.",
        "placeholder": ""
    },
    "ImageFormatRaw.pixelFormat.yuv420p": {
        "title": "Yuv420p",
        "description": "Planar YUV 4:2:0, 12bpp, (1 Cr and Cb sample per 2x2 Y samples)."
    },
    "ImageFormatRaw.pixelFormat.rgb565be": {
        "title": "Rgb565be",
        "description": "Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), big-endian."
    },
    "ImageFormatRaw.pixelFormat.rgb565le": {
        "title": "Rgb565le",
        "description": "Packed RGB 5:6:5, 16bpp, (msb)   5R 6G 5B(lsb), little-endian."
    },
    "ImageFormatRaw.pixelFormat.rgb555be": {
        "title": "Rgb555be",
        "description": "Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), big-endian , X=unused/undefined."
    },
    "ImageFormatRaw.pixelFormat.rgb555le": {
        "title": "Rgb555le",
        "description": "Packed RGB 5:5:5, 16bpp, (msb)1X 5R 5G 5B(lsb), little-endian, X=unused/undefined."
    },
    "ImageFormatRaw.pixelFormat.rgb24": {
        "title": "Rgb24",
        "description": "Packed RGB 8:8:8, 24bpp, RGBRGB."
    },
    "ImageFormatRaw.pixelFormat.bgr24": {
        "title": "Bgr24",
        "description": "Packed RGB 8:8:8, 24bpp, BGRBGR."
    },
    "ImageFormatRaw.pixelFormat.argb": {
        "title": "Argb",
        "description": "Packed ARGB 8:8:8:8, 32bpp, ARGBARGB."
    },
    "ImageFormatRaw.pixelFormat.rgba": {
        "title": "Rgba",
        "description": "Packed RGBA 8:8:8:8, 32bpp, RGBARGBA."
    },
    "ImageFormatRaw.pixelFormat.abgr": {
        "title": "Abgr",
        "description": "Packed ABGR 8:8:8:8, 32bpp, ABGRABGR."
    },
    "ImageFormatRaw.pixelFormat.bgra": {
        "title": "Bgra",
        "description": "Packed BGRA 8:8:8:8, 32bpp, BGRABGRA."
    },
    "ImageFormatJpeg": {
        "title": "Image format jpeg",
        "description": "JPEG image encoding."
    },
    "ImageFormatJpeg.quality": {
        "title": "Quality",
        "description": "Image quality value between 0 to 100 (best quality).",
        "placeholder": ""
    },
    "ImageFormatBmp": {
        "title": "Image format bmp",
        "description": "BMP image encoding."
    },
    "ImageFormatPng": {
        "title": "Image format png",
        "description": "PNG image encoding."
    },
    "NamedLineBase": {
        "title": "Named line base",
        "description": "Base class for named lines."
    },
    "NamedLineBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "NamedLineBase.name": {
        "title": "Name",
        "description": "Line name. Must be unique within the node.",
        "placeholder": ""
    },
    "NamedLineString": {
        "title": "Named line string",
        "description": "Describes a line configuration."
    },
    "NamedLineString.line": {
        "title": "Line",
        "description": "Point coordinates for the line start and end, respectively. Example: '[[0.3, 0.2],[0.9, 0.8]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner.",
        "placeholder": "[[0.3,0.2],[0.9,0.8]]"
    },
    "NamedPolygonBase": {
        "title": "Named polygon base",
        "description": "Describes the named polygon."
    },
    "NamedPolygonBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "NamedPolygonBase.name": {
        "title": "Name",
        "description": "Polygon name. Must be unique within the node.",
        "placeholder": ""
    },
    "NamedPolygonString": {
        "title": "Named polygon string",
        "description": "Describes a closed polygon configuration."
    },
    "NamedPolygonString.polygon": {
        "title": "Polygon",
        "description": "Point coordinates for the polygon. Example: '[[0.3, 0.2],[0.9, 0.8],[0.7, 0.6]]'. Each point is expressed as [LEFT, TOP] coordinate ratios ranging from 0.0 to 1.0, where [0,0] is the upper-left frame corner and [1, 1] is the bottom-right frame corner.",
        "placeholder": "[[0.3, 0.2],[0.9, 0.8],[0.7, 0.6]]"
    },
    "SignalGateProcessor": {
        "title": "Signal gate processor",
        "description": "A signal gate determines when to block (gate) incoming media, and when to allow it through. It gathers input events over the activationEvaluationWindow, and determines whether to open or close the gate. See https://aka.ms/ava-signalgate for more information."
    },
    "SignalGateProcessor.activationEvaluationWindow": {
        "title": "Activation evaluation window",
        "description": "The period of time over which the gate gathers input events before evaluating them.",
        "placeholder": "PT1.0S"
    },
    "SignalGateProcessor.activationSignalOffset": {
        "title": "Activation signal offset",
        "description": "Signal offset once the gate is activated (can be negative). It determines the how much farther behind of after the signal will be let through based on the activation time. A negative offset indicates that data prior the activation time must be included on the signal that is let through, once the gate is activated. When used upstream of a file or video sink, this allows for scenarios such as recording buffered media prior an event, such as: record video 5 seconds prior motions is detected.",
        "placeholder": "-PT1.0S"
    },
    "SignalGateProcessor.minimumActivationTime": {
        "title": "Minimum activation time",
        "description": "The minimum period for which the gate remains open in the absence of subsequent triggers (events). When used upstream of a file or video sink, it determines the minimum length of the recorded video clip.",
        "placeholder": "PT1S"
    },
    "SignalGateProcessor.maximumActivationTime": {
        "title": "Maximum activation time",
        "description": "The maximum period for which the gate remains open in the presence of subsequent triggers (events). When used upstream of a file or video sink, it determines the maximum length of the recorded video clip.",
        "placeholder": "PT2S"
    },
    "SpatialAnalysisOperationBase": {
        "title": "Spatial analysis operation base",
        "description": "Base class for Azure Cognitive Services Spatial Analysis operations."
    },
    "SpatialAnalysisOperationBase.@type": {
        "title": "@type",
        "description": "The Type discriminator for the derived types.",
        "placeholder": ""
    },
    "SpatialAnalysisCustomOperation": {
        "title": "Spatial analysis custom operation",
        "description": "Defines a Spatial Analysis custom operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisCustomOperation.extensionConfiguration": {
        "title": "Extension configuration",
        "description": "Custom configuration to pass to the Azure Cognitive Services Spatial Analysis module.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase": {
        "title": "Spatial analysis typed operation base",
        "description": "Base class for Azure Cognitive Services Spatial Analysis typed operations."
    },
    "SpatialAnalysisTypedOperationBase.debug": {
        "title": "Debug",
        "description": "If set to 'true', enables debugging mode for this operation.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.calibrationConfiguration": {
        "title": "Calibration configuration",
        "description": "Advanced calibration configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.cameraConfiguration": {
        "title": "Camera configuration",
        "description": "Advanced camera configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.cameraCalibratorNodeConfiguration": {
        "title": "Camera calibrator node configuration",
        "description": "Advanced camera calibrator configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.detectorNodeConfiguration": {
        "title": "Detector node configuration",
        "description": "Advanced detector node configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.trackerNodeConfiguration": {
        "title": "Tracker node configuration",
        "description": "Advanced tracker node configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisTypedOperationBase.enableFaceMaskClassifier": {
        "title": "Enable face mask classifier",
        "description": "If set to 'true', enables face mask detection for this operation.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase": {
        "title": "Spatial analysis operation event base",
        "description": "Defines the Azure Cognitive Services Spatial Analysis operation eventing configuration."
    },
    "SpatialAnalysisOperationEventBase.threshold": {
        "title": "Threshold",
        "description": "The event threshold.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase.focus": {
        "title": "Focus",
        "description": "The operation focus type.",
        "placeholder": ""
    },
    "SpatialAnalysisOperationEventBase.focus.center": {
        "title": "Center",
        "description": "The center of the object."
    },
    "SpatialAnalysisOperationEventBase.focus.bottomCenter": {
        "title": "Bottom center",
        "description": "The bottom center of the object."
    },
    "SpatialAnalysisOperationEventBase.focus.footprint": {
        "title": "Footprint",
        "description": "The footprint."
    },
    "SpatialAnalysisPersonCountEvent": {
        "title": "Spatial analysis person count event",
        "description": "Defines a Spatial Analysis person count operation eventing configuration."
    },
    "SpatialAnalysisPersonCountEvent.trigger": {
        "title": "Trigger",
        "description": "The event trigger type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountEvent.trigger.event": {
        "title": "Event",
        "description": "Event trigger."
    },
    "SpatialAnalysisPersonCountEvent.trigger.interval": {
        "title": "Interval",
        "description": "Interval trigger."
    },
    "SpatialAnalysisPersonCountEvent.outputFrequency": {
        "title": "Output frequency",
        "description": "The event or interval output frequency.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonCountOperation": {
        "title": "Spatial analysis person count operation",
        "description": "Defines a Spatial Analysis person count operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonCountOperation.zones": {
        "title": "Zones",
        "description": "The list of zones and optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingEvent": {
        "title": "Spatial analysis person zone crossing event",
        "description": "Defines a Spatial Analysis person crossing zone operation eventing configuration."
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType": {
        "title": "Event type",
        "description": "The event type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType.zoneCrossing": {
        "title": "Zone crossing",
        "description": "Zone crossing event type."
    },
    "SpatialAnalysisPersonZoneCrossingEvent.eventType.zoneDwellTime": {
        "title": "Zone dwell time",
        "description": "Zone dwell time event type."
    },
    "SpatialAnalysisPersonZoneCrossingZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonZoneCrossingOperation": {
        "title": "Spatial analysis person zone crossing operation",
        "description": "Defines a Spatial Analysis person zone crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonZoneCrossingOperation.zones": {
        "title": "Zones",
        "description": "The list of zones with optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent": {
        "title": "Spatial analysis person distance event",
        "description": "Defines a Spatial Analysis person distance operation eventing configuration."
    },
    "SpatialAnalysisPersonDistanceEvent.trigger": {
        "title": "Trigger",
        "description": "The event trigger type.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.trigger.event": {
        "title": "Event",
        "description": "Event trigger."
    },
    "SpatialAnalysisPersonDistanceEvent.trigger.interval": {
        "title": "Interval",
        "description": "Interval trigger."
    },
    "SpatialAnalysisPersonDistanceEvent.outputFrequency": {
        "title": "Output frequency",
        "description": "The event or interval output frequency.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.minimumDistanceThreshold": {
        "title": "Minimum distance threshold",
        "description": "The minimum distance threshold",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceEvent.maximumDistanceThreshold": {
        "title": "Maximum distance threshold",
        "description": "The maximum distance threshold",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceZoneEvents.zone": {
        "title": "Zone",
        "description": "The named zone.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceZoneEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonDistanceOperation": {
        "title": "Spatial analysis person distance operation",
        "description": "Defines a Spatial Analysis person distance operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonDistanceOperation.zones": {
        "title": "Zones",
        "description": "The list of zones with optional events.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingEvent": {
        "title": "Spatial analysis person line crossing event",
        "description": "Defines a Spatial Analysis person line crossing operation eventing configuration."
    },
    "SpatialAnalysisPersonLineCrossingLineEvents.line": {
        "title": "Line",
        "description": "The named line.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingLineEvents.events": {
        "title": "Events",
        "description": "The event configuration.",
        "placeholder": ""
    },
    "SpatialAnalysisPersonLineCrossingOperation": {
        "title": "Spatial analysis person line crossing operation",
        "description": "Defines a Spatial Analysis person line crossing operation. This requires the Azure Cognitive Services Spatial analysis module to be deployed alongside the Video Analyzer module, please see https://aka.ms/ava-spatial-analysis for more information."
    },
    "SpatialAnalysisPersonLineCrossingOperation.lines": {
        "title": "Lines",
        "description": "The list of lines with optional events.",
        "placeholder": ""
    },
    "RemoteDeviceAdapter": {
        "title": "Remote device adapter",
        "description": "The Video Analyzer edge module can act as a transparent gateway for video, enabling IoT devices to send video to the cloud from behind a firewall. A remote device adapter should be created for each such IoT device. Communication between the cloud and IoT device would then flow via the Video Analyzer edge module."
    },
    "RemoteDeviceAdapter.name": {
        "title": "Name",
        "description": "The unique identifier for the remote device adapter.",
        "placeholder": ""
    },
    "RemoteDeviceAdapter.systemData": {
        "title": "System data",
        "description": "Read-only system metadata associated with this object.",
        "placeholder": ""
    },
    "RemoteDeviceAdapter.properties": {
        "title": "Properties",
        "description": "Properties of the remote device adapter.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterProperties": {
        "title": "Remote device adapter properties",
        "description": "Remote device adapter properties."
    },
    "RemoteDeviceAdapterProperties.description": {
        "title": "Description",
        "description": "An optional description for the remote device adapter.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterProperties.target": {
        "title": "Target",
        "description": "The IoT device to which this remote device will connect.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterProperties.iotHubDeviceConnection": {
        "title": "IoT hub device connection",
        "description": "Information that enables communication between the IoT Hub and the IoT device - allowing this edge module to act as a transparent gateway between the two.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterTarget": {
        "title": "Remote device adapter target",
        "description": "Properties of the remote device adapter target."
    },
    "RemoteDeviceAdapterTarget.host": {
        "title": "Host",
        "description": "Hostname or IP address of the remote device.",
        "placeholder": ""
    },
    "IotHubDeviceConnection": {
        "title": "IoT hub device connection",
        "description": "Information that enables communication between the IoT Hub and the IoT device - allowing this edge module to act as a transparent gateway between the two."
    },
    "IotHubDeviceConnection.deviceId": {
        "title": "Device id",
        "description": "The name of the IoT device configured and managed in IoT Hub. (case-sensitive)",
        "placeholder": ""
    },
    "IotHubDeviceConnection.credentials": {
        "title": "Credentials",
        "description": "IoT device connection credentials. Currently IoT device symmetric key credentials are supported.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterCollection": {
        "title": "Remote device adapter collection",
        "description": "A list of remote device adapters."
    },
    "RemoteDeviceAdapterCollection.value": {
        "title": "Value",
        "description": "An array of remote device adapters.",
        "placeholder": ""
    },
    "RemoteDeviceAdapterCollection.@continuationToken": {
        "title": "@continuation token",
        "description": "A continuation token to use in subsequent calls to enumerate through the remote device adapter collection. This is used when the collection contains too many results to return in one response.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor": {
        "title": "Cognitive services vision processor",
        "description": "A processor that allows the pipeline topology to send video frames to a Cognitive Services Vision extension. Inference results are relayed to downstream nodes."
    },
    "CognitiveServicesVisionProcessor.endpoint": {
        "title": "Endpoint",
        "description": "Endpoint to which this processor should connect.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.image": {
        "title": "Image",
        "description": "Describes the parameters of the image that is sent as input to the endpoint.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.samplingOptions": {
        "title": "Sampling options",
        "description": "Describes the sampling options to be applied when forwarding samples to the extension.",
        "placeholder": ""
    },
    "CognitiveServicesVisionProcessor.operation": {
        "title": "Operation",
        "description": "Describes the Spatial Analysis operation to be used in the Cognitive Services Vision processor.",
        "placeholder": ""
    },
    "DiscoveredOnvifDeviceCollection": {
        "title": "Discovered onvif device collection",
        "description": "A list of ONVIF devices that were discovered in the same subnet as the IoT Edge device."
    },
    "DiscoveredOnvifDeviceCollection.value": {
        "title": "Value",
        "description": "An array of ONVIF devices that have been discovered in the same subnet as the IoT Edge device.",
        "placeholder": ""
    },
    "DiscoveredOnvifDevice": {
        "title": "Discovered onvif device",
        "description": "The discovered properties of the ONVIF device that are returned during the discovery."
    },
    "DiscoveredOnvifDevice.serviceIdentifier": {
        "title": "Service identifier",
        "description": "The unique identifier of the ONVIF device that was discovered in the same subnet as the IoT Edge device.",
        "placeholder": ""
    },
    "DiscoveredOnvifDevice.remoteIPAddress": {
        "title": "Remote i p address",
        "description": "The IP address of the ONVIF device that was discovered in the same subnet as the IoT Edge device.",
        "placeholder": ""
    },
    "DiscoveredOnvifDevice.scopes": {
        "title": "Scopes",
        "description": "An array of hostnames for the ONVIF discovered devices that are in the same subnet as the IoT Edge device.",
        "placeholder": ""
    },
    "DiscoveredOnvifDevice.endpoints": {
        "title": "Endpoints",
        "description": "An array of media profile endpoints that the ONVIF discovered device supports.",
        "placeholder": ""
    },
    "OnvifDevice": {
        "title": "Onvif device",
        "description": "The ONVIF device properties."
    },
    "OnvifDevice.hostname": {
        "title": "Hostname",
        "description": "The hostname of the ONVIF device.",
        "placeholder": ""
    },
    "OnvifDevice.systemDateTime": {
        "title": "System date time",
        "description": "The system date and time of the ONVIF device.",
        "placeholder": ""
    },
    "OnvifDevice.dns": {
        "title": "Dns",
        "description": "The ONVIF device DNS properties.",
        "placeholder": ""
    },
    "OnvifDevice.mediaProfiles": {
        "title": "Media profiles",
        "description": "An array of of ONVIF media profiles supported by the ONVIF device.",
        "placeholder": ""
    },
    "OnvifSystemDateTime": {
        "title": "Onvif system date time",
        "description": "The ONVIF device DNS properties."
    },
    "OnvifSystemDateTime.type": {
        "title": "Type",
        "description": "An enum value determining whether the date time was configured using NTP or manual.",
        "placeholder": ""
    },
    "OnvifSystemDateTime.time": {
        "title": "Time",
        "description": "The device datetime returned when calling the request.",
        "placeholder": ""
    },
    "OnvifSystemDateTime.timeZone": {
        "title": "Time zone",
        "description": "The timezone of the ONVIF device datetime.",
        "placeholder": ""
    },
    "OnvifHostName": {
        "title": "Onvif host name",
        "description": "The ONVIF device DNS properties."
    },
    "OnvifHostName.fromDhcp": {
        "title": "From dhcp",
        "description": "Result value showing if the ONVIF device is configured to use DHCP.",
        "placeholder": ""
    },
    "OnvifHostName.hostname": {
        "title": "Hostname",
        "description": "The hostname of the ONVIF device.",
        "placeholder": ""
    },
    "OnvifDns": {
        "title": "Onvif dns",
        "description": "The ONVIF device DNS properties."
    },
    "OnvifDns.fromDhcp": {
        "title": "From dhcp",
        "description": "Result value showing if the ONVIF device is configured to use DHCP.",
        "placeholder": ""
    },
    "OnvifDns.ipv4Address": {
        "title": "Ipv4 address",
        "description": "An array of IPv4 address for the discovered ONVIF device.",
        "placeholder": ""
    },
    "OnvifDns.ipv6Address": {
        "title": "Ipv6 address",
        "description": "An array of IPv6 address for the discovered ONVIF device.",
        "placeholder": ""
    },
    "MediaProfile": {
        "title": "Media profile",
        "description": "Class representing the ONVIF MediaProfiles."
    },
    "MediaProfile.name": {
        "title": "Name",
        "description": "The name of the Media Profile.",
        "placeholder": ""
    },
    "MediaProfile.mediaUri": {
        "title": "Media URI",
        "description": "Object representing the URI that will be used to request for media streaming.",
        "placeholder": ""
    },
    "MediaProfile.videoEncoderConfiguration": {
        "title": "Video encoder configuration",
        "description": "The Video encoder configuration.",
        "placeholder": ""
    },
    "MediaUri": {
        "title": "Media URI",
        "description": "Object representing the URI that will be used to request for media streaming."
    },
    "MediaUri.uri": {
        "title": "URI",
        "description": "URI that can be used for media streaming.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration": {
        "title": "Video encoder configuration",
        "description": "Class representing the MPEG4 Configuration."
    },
    "VideoEncoderConfiguration.encoding": {
        "title": "Encoding",
        "description": "The video codec used by the Media Profile.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration.encoding.JPEG": {
        "title": "J p e g",
        "description": "The Media Profile uses JPEG encoding."
    },
    "VideoEncoderConfiguration.encoding.H264": {
        "title": "H264",
        "description": "The Media Profile uses H264 encoding."
    },
    "VideoEncoderConfiguration.encoding.MPEG4": {
        "title": "M p e g4",
        "description": "The Media Profile uses MPEG4 encoding."
    },
    "VideoEncoderConfiguration.quality": {
        "title": "Quality",
        "description": "Relative value representing the quality of the video.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration.resolution": {
        "title": "Resolution",
        "description": "The Video Resolution.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration.rateControl": {
        "title": "Rate control",
        "description": "The Video's rate control.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration.h264": {
        "title": "H264",
        "description": "The H264 Configuration.",
        "placeholder": ""
    },
    "VideoEncoderConfiguration.mpeg4": {
        "title": "Mpeg4",
        "description": "The H264 Configuration.",
        "placeholder": ""
    },
    "MPEG4Configuration": {
        "title": "M p e g4 configuration",
        "description": "Class representing the MPEG4 Configuration."
    },
    "MPEG4Configuration.govLength": {
        "title": "Gov length",
        "description": "Group of Video frames length.",
        "placeholder": ""
    },
    "MPEG4Configuration.profile": {
        "title": "Profile",
        "description": "The MPEG4 Profile",
        "placeholder": ""
    },
    "MPEG4Configuration.profile.SP": {
        "title": "S p",
        "description": "Simple Profile."
    },
    "MPEG4Configuration.profile.ASP": {
        "title": "A s p",
        "description": "Advanced Simple Profile."
    },
    "RateControl": {
        "title": "Rate control",
        "description": "Class  representing the video's rate control."
    },
    "RateControl.bitRateLimit": {
        "title": "Bit rate limit",
        "description": "the maximum output bitrate in kbps.",
        "placeholder": ""
    },
    "RateControl.encodingInterval": {
        "title": "Encoding interval",
        "description": "Interval at which images are encoded and transmitted.",
        "placeholder": ""
    },
    "RateControl.frameRateLimit": {
        "title": "Frame rate limit",
        "description": "Maximum output framerate in fps.",
        "placeholder": ""
    },
    "RateControl.guaranteedFrameRate": {
        "title": "Guaranteed frame rate",
        "description": "A value of true indicates that frame rate is a fixed value rather than an upper limit, and that the video encoder shall prioritize frame rate over all other adaptable configuration values such as bitrate.",
        "placeholder": ""
    },
    "VideoResolution": {
        "title": "Video resolution",
        "description": "The Video resolution."
    },
    "VideoResolution.width": {
        "title": "Width",
        "description": "The number of columns of the Video image.",
        "placeholder": ""
    },
    "VideoResolution.height": {
        "title": "Height",
        "description": "The number of lines of the Video image.",
        "placeholder": ""
    },
    "H264Configuration": {
        "title": "H264 configuration",
        "description": "Class representing the H264 Configuration."
    },
    "H264Configuration.govLength": {
        "title": "Gov length",
        "description": "Group of Video frames length.",
        "placeholder": ""
    },
    "H264Configuration.profile": {
        "title": "Profile",
        "description": "The H264 Profile",
        "placeholder": ""
    }
}